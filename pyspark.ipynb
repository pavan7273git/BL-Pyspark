{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Pyspark Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'pyspark.version' from 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\pyspark\\\\version.py'>\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-I3CDTH6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2b3fe9bbd10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+---------+------+---------+----------+-------------+------------------+---------------------+----------------------+----------------+\n",
      "|      Date|Confirmed|Deaths|Recovered|Active|New cases|New deaths|New recovered|Deaths / 100 Cases|Recovered / 100 Cases|Deaths / 100 Recovered|No. of countries|\n",
      "+----------+---------+------+---------+------+---------+----------+-------------+------------------+---------------------+----------------------+----------------+\n",
      "|2020-01-22|      555|    17|       28|   510|        0|         0|            0|              3.06|                 5.05|                 60.71|               6|\n",
      "|2020-01-23|      654|    18|       30|   606|       99|         1|            2|              2.75|                 4.59|                  60.0|               8|\n",
      "|2020-01-24|      941|    26|       36|   879|      287|         8|            6|              2.76|                 3.83|                 72.22|               9|\n",
      "|2020-01-25|     1434|    42|       39|  1353|      493|        16|            3|              2.93|                 2.72|                107.69|              11|\n",
      "|2020-01-26|     2118|    56|       52|  2010|      684|        14|           13|              2.64|                 2.46|                107.69|              13|\n",
      "|2020-01-27|     2927|    82|       61|  2784|      809|        26|            9|               2.8|                 2.08|                134.43|              16|\n",
      "|2020-01-28|     5578|   131|      107|  5340|     2651|        49|           46|              2.35|                 1.92|                122.43|              16|\n",
      "|2020-01-29|     6166|   133|      125|  5908|      588|         2|           18|              2.16|                 2.03|                 106.4|              18|\n",
      "|2020-01-30|     8234|   171|      141|  7922|     2068|        38|           16|              2.08|                 1.71|                121.28|              20|\n",
      "|2020-01-31|     9927|   213|      219|  9495|     1693|        42|           78|              2.15|                 2.21|                 97.26|              24|\n",
      "|2020-02-01|    12038|   259|      281| 11498|     2111|        46|           62|              2.15|                 2.33|                 92.17|              25|\n",
      "|2020-02-02|    16787|   362|      459| 15966|     4749|       103|          178|              2.16|                 2.73|                 78.87|              25|\n",
      "|2020-02-03|    19887|   426|      604| 18857|     3100|        64|          145|              2.14|                 3.04|                 70.53|              25|\n",
      "|2020-02-04|    23898|   492|      821| 22585|     4011|        66|          217|              2.06|                 3.44|                 59.93|              26|\n",
      "|2020-02-05|    27643|   564|     1071| 26008|     3745|        72|          250|              2.04|                 3.87|                 52.66|              26|\n",
      "|2020-02-06|    30802|   634|     1418| 28750|     3159|        70|          347|              2.06|                  4.6|                 44.71|              26|\n",
      "|2020-02-07|    34334|   719|     1903| 31712|     3532|        85|          485|              2.09|                 5.54|                 37.78|              26|\n",
      "|2020-02-08|    37068|   806|     2470| 33792|     2734|        87|          567|              2.17|                 6.66|                 32.63|              26|\n",
      "|2020-02-09|    40095|   906|     3057| 36132|     3027|       100|          587|              2.26|                 7.62|                 29.64|              26|\n",
      "|2020-02-10|    42633|  1013|     3714| 37906|     2538|       107|          657|              2.38|                 8.71|                 27.28|              26|\n",
      "+----------+---------+------+---------+------+---------+----------+-------------+------------------+---------------------+----------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=spark.read.csv(r\"C:\\Users\\Lenovo\\Downloads\\dyd\\data\\day_wise.csv\",header=True,inferSchema=True)\n",
    "\n",
    "df.show() #it shows the rows but by default it shows only top 20 rows\n",
    "\n",
    "df.count() #this will count the number of rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+---------+-------+---------+----------+-------------+------------------+---------------------+----------------------+----------------+\n",
      "|      Date|Confirmed|Deaths|Recovered| Active|New cases|New deaths|New recovered|Deaths / 100 Cases|Recovered / 100 Cases|Deaths / 100 Recovered|No. of countries|\n",
      "+----------+---------+------+---------+-------+---------+----------+-------------+------------------+---------------------+----------------------+----------------+\n",
      "|2020-02-05|    27643|   564|     1071|  26008|     3745|        72|          250|              2.04|                 3.87|                 52.66|              26|\n",
      "|2020-02-21|    76206|  2250|    17966|  55990|      629|         4|          597|              2.95|                23.58|                 12.52|              30|\n",
      "|2020-03-31|   871355| 44478|   174074| 652803|    76416|      4844|        12367|               5.1|                19.98|                 25.55|             179|\n",
      "|2020-04-10|  1671907|108551|   367477|1195879|    87658|      7272|        21128|              6.49|                21.98|                 29.54|             184|\n",
      "|2020-04-16|  2162715|148591|   529015|1485109|    96712|      7283|        30090|              6.87|                24.46|                 28.09|             184|\n",
      "|2020-04-24|  2806267|201401|   771329|1833537|    96974|      6674|        49640|              7.18|                27.49|                 26.11|             184|\n",
      "|2020-05-11|  4180268|287608|  1416204|2476456|    76298|      3473|        46096|              6.88|                33.88|                 20.31|             186|\n",
      "|2020-05-24|  5417579|346525|  2117555|2953499|    95326|      3140|        54753|               6.4|                39.09|                 16.36|             187|\n",
      "|2020-05-25|  5504542|347703|  2180605|2976234|    87335|      1178|        63050|              6.32|                39.61|                 15.95|             187|\n",
      "|2020-05-27|  5699664|357119|  2297613|3044932|   102600|      5213|        62495|              6.27|                40.31|                 15.54|             187|\n",
      "+----------+---------+------+---------+-------+---------+----------+-------------+------------------+---------------------+----------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv(r\"C:\\Users\\Lenovo\\Downloads\\dyd\\data\\day_wise.csv\",header=True,inferSchema=True)\n",
    "\n",
    "df.sample(fraction=0.08).show(10)  #here the fraction specifies that from what percent it return , we can give 0.1 to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+---------+-------+---------+----------+-------------+------------------+---------------------+----------------------+----------------+\n",
      "|      Date|Confirmed|Deaths|Recovered| Active|New cases|New deaths|New recovered|Deaths / 100 Cases|Recovered / 100 Cases|Deaths / 100 Recovered|No. of countries|\n",
      "+----------+---------+------+---------+-------+---------+----------+-------------+------------------+---------------------+----------------------+----------------+\n",
      "|2020-02-03|    19887|   426|      604|  18857|     3100|        64|          145|              2.14|                 3.04|                 70.53|              25|\n",
      "|2020-02-09|    40095|   906|     3057|  36132|     3027|       100|          587|              2.26|                 7.62|                 29.64|              26|\n",
      "|2020-02-14|    66690|  1523|     7613|  57554|     6484|       152|         1683|              2.28|                11.42|                 20.01|              27|\n",
      "|2020-02-15|    68765|  1666|     8902|  58197|     2075|       143|         1289|              2.42|                12.95|                 18.71|              27|\n",
      "|2020-02-21|    76206|  2250|    17966|  55990|      629|         4|          597|              2.95|                23.58|                 12.52|              30|\n",
      "|2020-03-05|    97331|  3342|    52237|  41752|     2791|        93|         2618|              3.43|                53.67|                   6.4|              84|\n",
      "|2020-04-10|  1671907|108551|   367477|1195879|    87658|      7272|        21128|              6.49|                21.98|                 29.54|             184|\n",
      "|2020-04-28|  3108149|221974|   884680|2001495|    75404|      6463|        32298|              7.14|                28.46|                 25.09|             184|\n",
      "|2020-05-03|  3515244|248659|  1097577|2169008|    77636|      3453|        31215|              7.07|                31.22|                 22.66|             186|\n",
      "|2020-05-08|  3941935|276304|  1284849|2380782|    92997|      5568|        35538|              7.01|                32.59|                  21.5|             186|\n",
      "+----------+---------+------+---------+-------+---------+----------+-------------+------------------+---------------------+----------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Confirmed: integer (nullable = true)\n",
      " |-- Deaths: integer (nullable = true)\n",
      " |-- Recovered: integer (nullable = true)\n",
      " |-- Active: integer (nullable = true)\n",
      " |-- New cases: integer (nullable = true)\n",
      " |-- New deaths: integer (nullable = true)\n",
      " |-- New recovered: integer (nullable = true)\n",
      " |-- Deaths / 100 Cases: double (nullable = true)\n",
      " |-- Recovered / 100 Cases: double (nullable = true)\n",
      " |-- Deaths / 100 Recovered: double (nullable = true)\n",
      " |-- No. of countries: integer (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv(r\"C:\\Users\\Lenovo\\Downloads\\dyd\\data\\day_wise.csv\",header=True,inferSchema=True)\n",
    "\n",
    "df.sample(fraction=0.08,seed=41).show(10)\n",
    "\n",
    "print(df.printSchema()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Date', 'date'),\n",
       " ('Confirmed', 'int'),\n",
       " ('Deaths', 'int'),\n",
       " ('Recovered', 'int'),\n",
       " ('Active', 'int'),\n",
       " ('New cases', 'int'),\n",
       " ('New deaths', 'int'),\n",
       " ('New recovered', 'int'),\n",
       " ('Deaths / 100 Cases', 'double'),\n",
       " ('Recovered / 100 Cases', 'double'),\n",
       " ('Deaths / 100 Recovered', 'double'),\n",
       " ('No. of countries', 'int')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=spark.read.csv(r\"C:\\Users\\Lenovo\\Downloads\\dyd\\data\\day_wise.csv\",header=True,inferSchema=True)\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date',\n",
       " 'Confirmed',\n",
       " 'Deaths',\n",
       " 'Recovered',\n",
       " 'Active',\n",
       " 'New cases',\n",
       " 'New deaths',\n",
       " 'New recovered',\n",
       " 'Deaths / 100 Cases',\n",
       " 'Recovered / 100 Cases',\n",
       " 'Deaths / 100 Recovered',\n",
       " 'No. of countries']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df=spark.read.csv(r\"C:\\Users\\Lenovo\\Downloads\\dyd\\data\\day_wise.csv\",header=True,inferSchema=True)\n",
    "\n",
    "\n",
    "df.columns #it will shows all the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, UID: string, iso2: string, iso3: string, code3: string, FIPS: string, Admin2: string, Province_State: string, Country_Region: string, Lat: string, Long_: string, Combined_Key: string, Date: string, Confirmed: string, Deaths: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=spark.read.csv(\"C:/Users/Lenovo/Downloads/dyd/data/usa_county_wise.csv\",header=True,inferSchema=True)\n",
    "\n",
    "#df.describe(\"Confirmed\", \"Deaths\", \"Recovered\", \"Active\").show(5, truncate=False)\n",
    "\n",
    "#df.select(\"Confirmed\").summary(\"count\", \"mean\", \"min\", \"max\").show()\n",
    "\n",
    "df.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+------+------------------+------------------+---------+--------------+--------------+-------------------+------------------+--------------------+-------+------------------+-----------------+\n",
      "|summary|                UID|  iso2|  iso3|             code3|              FIPS|   Admin2|Province_State|Country_Region|                Lat|             Long_|        Combined_Key|   Date|         Confirmed|           Deaths|\n",
      "+-------+-------------------+------+------+------------------+------------------+---------+--------------+--------------+-------------------+------------------+--------------------+-------+------------------+-----------------+\n",
      "|  count|             627920|627920|627920|            627920|            626040|   626792|        627920|        627920|             627920|            627920|              627920| 627920|            627920|           627920|\n",
      "|   mean|8.342957994550899E7|  NULL|  NULL|  834.491616766467| 33061.68468468468|     NULL|          NULL|          NULL|  36.70721244570872|-88.60147445561189|                NULL|   NULL|357.28428462224485|17.53632787616257|\n",
      "| stddev|  4314702.339347854|  NULL|  NULL|36.492620240099704|18636.156825347312|     NULL|          NULL|          NULL|   9.06157189067228|21.715747344705864|                NULL|   NULL| 3487.282693522076|300.9914658104832|\n",
      "|    min|                 16|    AS|   ASM|                16|              60.0|Abbeville|       Alabama|            US|-14.270999999999999|         -174.1596|Abbeville, South ...|1/22/20|                 0|                0|\n",
      "|    max|           84099999|    VI|   VIR|               850|           99999.0|  Ziebach|       Wyoming|            US|        69.31479216|          145.6739|Ziebach, South Da...| 7/9/20|            224051|            23500|\n",
      "+-------+-------------------+------+------+------------------+------------------+---------+--------------+--------------+-------------------+------------------+--------------------+-------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary = df.describe(\"summary\", \"UID\", \"iso2\", \"iso3\").collect()\n",
    "# for row in summary:\n",
    "#     print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from pyspark.sql import SparkSession, Row\n",
    "\n",
    "# spark=SparkSession.builder.appName(\"Rows\").getOrCreate()\n",
    "\n",
    "# data=[Row(name=\"Pavan\",age=22),Row(name=\"Kumar\",age=21)]\n",
    "# df=spark.createDataFrame(data)\n",
    "\n",
    "#  df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from pyspark.sql import SparkSession, Row\n",
    "\n",
    "# spark = SparkSession.builder.appName(\"RowExample\").getOrCreate()\n",
    "\n",
    "\n",
    "# data1=[(\"Arun\",20),(\"Bib\",22)]\n",
    "# columns=[\"Name\",\"Age\"]\n",
    "\n",
    "# df1=spark.createDataFrame(data1,columns)\n",
    "# df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+------+------------------+------------------+---------+--------------+--------------+-------------------+------------------+--------------------+-------+------------------+-----------------+\n",
      "|summary|                UID|  iso2|  iso3|             code3|              FIPS|   Admin2|Province_State|Country_Region|                Lat|             Long_|        Combined_Key|   Date|         Confirmed|           Deaths|\n",
      "+-------+-------------------+------+------+------------------+------------------+---------+--------------+--------------+-------------------+------------------+--------------------+-------+------------------+-----------------+\n",
      "|  count|             627920|627920|627920|            627920|            626040|   626792|        627920|        627920|             627920|            627920|              627920| 627920|            627920|           627920|\n",
      "|   mean|8.342957994550899E7|  NULL|  NULL|  834.491616766467| 33061.68468468468|     NULL|          NULL|          NULL|  36.70721244570872|-88.60147445561189|                NULL|   NULL|357.28428462224485|17.53632787616257|\n",
      "| stddev|  4314702.339347854|  NULL|  NULL|36.492620240099704|18636.156825347312|     NULL|          NULL|          NULL|   9.06157189067228|21.715747344705864|                NULL|   NULL| 3487.282693522076|300.9914658104832|\n",
      "|    min|                 16|    AS|   ASM|                16|              60.0|Abbeville|       Alabama|            US|-14.270999999999999|         -174.1596|Abbeville, South ...|1/22/20|                 0|                0|\n",
      "|    max|           84099999|    VI|   VIR|               850|           99999.0|  Ziebach|       Wyoming|            US|        69.31479216|          145.6739|Ziebach, South Da...| 7/9/20|            224051|            23500|\n",
      "+-------+-------------------+------+------+------------------+------------------+---------+--------------+--------------+-------------------+------------------+--------------------+-------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new=spark.read.csv(\"C:/Users/Lenovo/Downloads/dyd/data/usa_county_wise.csv\",header=True,inferSchema=True)\n",
    "\n",
    "df_new.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Read Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|     UID|\n",
      "+--------+\n",
      "|      16|\n",
      "|     316|\n",
      "|     580|\n",
      "|63072001|\n",
      "|63072003|\n",
      "|63072005|\n",
      "|63072007|\n",
      "|63072009|\n",
      "|63072011|\n",
      "|63072013|\n",
      "|63072015|\n",
      "|63072017|\n",
      "|63072019|\n",
      "|63072021|\n",
      "|63072023|\n",
      "|63072025|\n",
      "|63072027|\n",
      "|63072029|\n",
      "|63072031|\n",
      "|63072033|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.select(\"UID\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+-----+-------+---------+--------------+--------------+-------------------+------------------+--------------------+-------+---------+------+\n",
      "|     UID|iso2|iso3|code3|   FIPS|   Admin2|Province_State|Country_Region|                Lat|             Long_|        Combined_Key|   Date|Confirmed|Deaths|\n",
      "+--------+----+----+-----+-------+---------+--------------+--------------+-------------------+------------------+--------------------+-------+---------+------+\n",
      "|      16|  AS| ASM|   16|   60.0|     NULL|American Samoa|            US|-14.270999999999999|          -170.132|  American Samoa, US|1/22/20|        0|     0|\n",
      "|63072001|  PR| PRI|  630|72001.0| Adjuntas|   Puerto Rico|            US| 18.180117000000006|        -66.754367|Adjuntas, Puerto ...|1/22/20|        0|     0|\n",
      "|63072003|  PR| PRI|  630|72003.0|   Aguada|   Puerto Rico|            US|          18.360255|-67.17513100000001|Aguada, Puerto Ri...|1/22/20|        0|     0|\n",
      "|63072005|  PR| PRI|  630|72005.0|Aguadilla|   Puerto Rico|            US|          18.459681|-67.12081500000001|Aguadilla, Puerto...|1/22/20|        0|     0|\n",
      "|63072015|  PR| PRI|  630|72015.0|   Arroyo|   Puerto Rico|            US| 17.998457000000005|        -66.056546|Arroyo, Puerto Ri...|1/22/20|        0|     0|\n",
      "+--------+----+----+-----+-------+---------+--------------+--------------+-------------------+------------------+--------------------+-------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.select(*df_new.columns).sample(fraction=0.5,seed=123).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+-----+-------+------------+--------------------+--------------+------------------+------------------+--------------------+-------+---------+------+\n",
      "|     UID|iso2|iso3|code3|   FIPS|      Admin2|      Province_State|Country_Region|               Lat|             Long_|        Combined_Key|   Date|Confirmed|Deaths|\n",
      "+--------+----+----+-----+-------+------------+--------------------+--------------+------------------+------------------+--------------------+-------+---------+------+\n",
      "|     580|  MP| MNP|  580|   69.0|        NULL|Northern Mariana ...|            US|           15.0979|          145.6739|Northern Mariana ...|1/22/20|        0|     0|\n",
      "|63072001|  PR| PRI|  630|72001.0|    Adjuntas|         Puerto Rico|            US|18.180117000000006|        -66.754367|Adjuntas, Puerto ...|1/22/20|        0|     0|\n",
      "|63072003|  PR| PRI|  630|72003.0|      Aguada|         Puerto Rico|            US|         18.360255|-67.17513100000001|Aguada, Puerto Ri...|1/22/20|        0|     0|\n",
      "|63072005|  PR| PRI|  630|72005.0|   Aguadilla|         Puerto Rico|            US|         18.459681|-67.12081500000001|Aguadilla, Puerto...|1/22/20|        0|     0|\n",
      "|63072007|  PR| PRI|  630|72007.0|Aguas Buenas|         Puerto Rico|            US|         18.251619|        -66.126806|Aguas Buenas, Pue...|1/22/20|        0|     0|\n",
      "|63072009|  PR| PRI|  630|72009.0|    Aibonito|         Puerto Rico|            US|         18.131361|        -66.264131|Aibonito, Puerto ...|1/22/20|        0|     0|\n",
      "|63072011|  PR| PRI|  630|72011.0|      Anasco|         Puerto Rico|            US|         18.287985|        -67.120611|Anasco, Puerto Ri...|1/22/20|        0|     0|\n",
      "|63072013|  PR| PRI|  630|72013.0|     Arecibo|         Puerto Rico|            US|         18.406631|        -66.675077|Arecibo, Puerto R...|1/22/20|        0|     0|\n",
      "|63072015|  PR| PRI|  630|72015.0|      Arroyo|         Puerto Rico|            US|17.998457000000005|        -66.056546|Arroyo, Puerto Ri...|1/22/20|        0|     0|\n",
      "|63072017|  PR| PRI|  630|72017.0| Barceloneta|         Puerto Rico|            US|18.445532999999998|-66.56053100000001|Barceloneta, Puer...|1/22/20|        0|     0|\n",
      "|63072019|  PR| PRI|  630|72019.0|Barranquitas|         Puerto Rico|            US|         18.201592|         -66.30963|Barranquitas, Pue...|1/22/20|        0|     0|\n",
      "|63072021|  PR| PRI|  630|72021.0|     Bayamon|         Puerto Rico|            US|          18.34946|        -66.168435|Bayamon, Puerto R...|1/22/20|        0|     0|\n",
      "|63072023|  PR| PRI|  630|72023.0|   Cabo Rojo|         Puerto Rico|            US|         18.040993|        -67.154391|Cabo Rojo, Puerto...|1/22/20|        0|     0|\n",
      "|63072025|  PR| PRI|  630|72025.0|      Caguas|         Puerto Rico|            US|         18.211615|        -66.050779|Caguas, Puerto Ri...|1/22/20|        0|     0|\n",
      "|63072027|  PR| PRI|  630|72027.0|       Camuy|         Puerto Rico|            US|         18.418578|        -66.860206|Camuy, Puerto Ric...|1/22/20|        0|     0|\n",
      "|63072029|  PR| PRI|  630|72029.0|   Canovanas|         Puerto Rico|            US|         18.328802|        -65.887612|Canovanas, Puerto...|1/22/20|        0|     0|\n",
      "|63072031|  PR| PRI|  630|72031.0|    Carolina|         Puerto Rico|            US|         18.374986|-65.95683100000001|Carolina, Puerto ...|1/22/20|        0|     0|\n",
      "|63072033|  PR| PRI|  630|72033.0|      Catano|         Puerto Rico|            US|         18.437269|-66.14330600000001|Catano, Puerto Ri...|1/22/20|        0|     0|\n",
      "|63072035|  PR| PRI|  630|72035.0|       Cayey|         Puerto Rico|            US|         18.102851|         -66.14914|Cayey, Puerto Ric...|1/22/20|        0|     0|\n",
      "|63072037|  PR| PRI|  630|72037.0|       Ceiba|         Puerto Rico|            US|         18.251818|        -65.666416|Ceiba, Puerto Ric...|1/22/20|        0|     0|\n",
      "+--------+----+----+-----+-------+------------+--------------------+--------------+------------------+------------------+--------------------+-------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.filter(df_new.UID>317).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UID: integer (nullable = true)\n",
      " |-- iso2: string (nullable = true)\n",
      " |-- iso3: string (nullable = true)\n",
      " |-- code3: integer (nullable = true)\n",
      " |-- FIPS: double (nullable = true)\n",
      " |-- Admin2: string (nullable = true)\n",
      " |-- Province_State: string (nullable = true)\n",
      " |-- Country_Region: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long_: double (nullable = true)\n",
      " |-- Combined_Key: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Confirmed: integer (nullable = true)\n",
      " |-- Deaths: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('UID', IntegerType(), True), StructField('iso2', StringType(), True), StructField('iso3', StringType(), True), StructField('code3', IntegerType(), True), StructField('FIPS', DoubleType(), True), StructField('Admin2', StringType(), True), StructField('Province_State', StringType(), True), StructField('Country_Region', StringType(), True), StructField('Lat', DoubleType(), True), StructField('Long_', DoubleType(), True), StructField('Combined_Key', StringType(), True), StructField('Date', StringType(), True), StructField('Confirmed', IntegerType(), True), StructField('Deaths', IntegerType(), True)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.printSchema()\n",
    "\n",
    "df_new.dtypes\n",
    "\n",
    "df_new.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|uid_number|\n",
      "+----------+\n",
      "|        16|\n",
      "|       316|\n",
      "|       580|\n",
      "|  63072001|\n",
      "|  63072003|\n",
      "|  63072005|\n",
      "|  63072007|\n",
      "|  63072009|\n",
      "|  63072011|\n",
      "|  63072013|\n",
      "|  63072015|\n",
      "|  63072017|\n",
      "|  63072019|\n",
      "|  63072021|\n",
      "|  63072023|\n",
      "|  63072025|\n",
      "|  63072027|\n",
      "|  63072029|\n",
      "|  63072031|\n",
      "|  63072033|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from  pyspark.sql.functions import col\n",
    "\n",
    "df_new.select(col(\"UID\").alias(\"uid_number\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------------+\n",
      "|     UID|added_code3|upper_provincestate|\n",
      "+--------+-----------+-------------------+\n",
      "|      16|         26|     AMERICAN SAMOA|\n",
      "|63072001|        640|        PUERTO RICO|\n",
      "|63072003|        640|        PUERTO RICO|\n",
      "|63072005|        640|        PUERTO RICO|\n",
      "|63072015|        640|        PUERTO RICO|\n",
      "+--------+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.selectExpr(\"UID\",\"CODE3 + 10 as added_code3\",\"upper(Province_state) as upper_provincestate\").sample(fraction=0.5,seed=123).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Filter comand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+-----+-------+--------+--------------------+--------------+------------------+------------------+--------------------+-------+---------+------+\n",
      "|     UID|iso2|iso3|code3|   FIPS|  Admin2|      Province_State|Country_Region|               Lat|             Long_|        Combined_Key|   Date|Confirmed|Deaths|\n",
      "+--------+----+----+-----+-------+--------+--------------------+--------------+------------------+------------------+--------------------+-------+---------+------+\n",
      "|     580|  MP| MNP|  580|   69.0|    NULL|Northern Mariana ...|            US|           15.0979|          145.6739|Northern Mariana ...|1/22/20|        0|     0|\n",
      "|63072001|  PR| PRI|  630|72001.0|Adjuntas|         Puerto Rico|            US|18.180117000000006|        -66.754367|Adjuntas, Puerto ...|1/22/20|        0|     0|\n",
      "|63072003|  PR| PRI|  630|72003.0|  Aguada|         Puerto Rico|            US|         18.360255|-67.17513100000001|Aguada, Puerto Ri...|1/22/20|        0|     0|\n",
      "+--------+----+----+-----+-------+--------+--------------------+--------------+------------------+------------------+--------------------+-------+---------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+\n",
      "|     UID|\n",
      "+--------+\n",
      "|     580|\n",
      "|63072001|\n",
      "|63072003|\n",
      "+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_new.filter(df_new.UID>317).show(3)\n",
    "\n",
    "df_new.selectExpr(\"UID\").where(df_new.UID>317).show(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "withcolumn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df2=df_new.withColumn(\"updated_code3\",col(\"code3\")+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "| new_deaths|     UID|\n",
      "+-----------+--------+\n",
      "|more deaths|84036061|\n",
      "|more deaths|84036061|\n",
      "|more deaths|84036061|\n",
      "|more deaths|84036061|\n",
      "|more deaths|84036061|\n",
      "|more deaths|84036061|\n",
      "|more deaths|84053033|\n",
      "|more deaths|84036061|\n",
      "|more deaths|84053033|\n",
      "|more deaths|84036061|\n",
      "|more deaths|84053033|\n",
      "|more deaths|84036061|\n",
      "|more deaths|84053033|\n",
      "|more deaths|84090038|\n",
      "|more deaths|84036061|\n",
      "|more deaths|84053033|\n",
      "|more deaths|84090038|\n",
      "|more deaths|84026163|\n",
      "|more deaths|84036061|\n",
      "|more deaths|84053033|\n",
      "+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when,col\n",
    "\n",
    "#here when() is not a method on DataFrame,\n",
    "#it is a function used inside .withColumn() or .selectExpr()  \n",
    "df3=df_new.withColumn(\"new_deaths\",when(col(\"Deaths\")>100,\"more deaths\").otherwise(\"less deaths\"))\n",
    "\n",
    "df3.where(col(\"new_deaths\") == \"more deaths\").select(\"new_deaths\",\"UID\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>literal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lit() stands for \"literal\", and it's used to add constant values into columns or expressions in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+-----+-------+------------+--------------------+--------------+-------------------+------------------+--------------------+-------+---------+------+-------+\n",
      "|     UID|iso2|iso3|code3|   FIPS|      Admin2|      Province_State|Country_Region|                Lat|             Long_|        Combined_Key|   Date|Confirmed|Deaths|country|\n",
      "+--------+----+----+-----+-------+------------+--------------------+--------------+-------------------+------------------+--------------------+-------+---------+------+-------+\n",
      "|      16|  AS| ASM|   16|   60.0|        NULL|      American Samoa|            US|-14.270999999999999|          -170.132|  American Samoa, US|1/22/20|        0|     0|  India|\n",
      "|     316|  GU| GUM|  316|   66.0|        NULL|                Guam|            US|            13.4443|          144.7937|            Guam, US|1/22/20|        0|     0|  India|\n",
      "|     580|  MP| MNP|  580|   69.0|        NULL|Northern Mariana ...|            US|            15.0979|          145.6739|Northern Mariana ...|1/22/20|        0|     0|  India|\n",
      "|63072001|  PR| PRI|  630|72001.0|    Adjuntas|         Puerto Rico|            US| 18.180117000000006|        -66.754367|Adjuntas, Puerto ...|1/22/20|        0|     0|  India|\n",
      "|63072003|  PR| PRI|  630|72003.0|      Aguada|         Puerto Rico|            US|          18.360255|-67.17513100000001|Aguada, Puerto Ri...|1/22/20|        0|     0|  India|\n",
      "|63072005|  PR| PRI|  630|72005.0|   Aguadilla|         Puerto Rico|            US|          18.459681|-67.12081500000001|Aguadilla, Puerto...|1/22/20|        0|     0|  India|\n",
      "|63072007|  PR| PRI|  630|72007.0|Aguas Buenas|         Puerto Rico|            US|          18.251619|        -66.126806|Aguas Buenas, Pue...|1/22/20|        0|     0|  India|\n",
      "|63072009|  PR| PRI|  630|72009.0|    Aibonito|         Puerto Rico|            US|          18.131361|        -66.264131|Aibonito, Puerto ...|1/22/20|        0|     0|  India|\n",
      "|63072011|  PR| PRI|  630|72011.0|      Anasco|         Puerto Rico|            US|          18.287985|        -67.120611|Anasco, Puerto Ri...|1/22/20|        0|     0|  India|\n",
      "|63072013|  PR| PRI|  630|72013.0|     Arecibo|         Puerto Rico|            US|          18.406631|        -66.675077|Arecibo, Puerto R...|1/22/20|        0|     0|  India|\n",
      "|63072015|  PR| PRI|  630|72015.0|      Arroyo|         Puerto Rico|            US| 17.998457000000005|        -66.056546|Arroyo, Puerto Ri...|1/22/20|        0|     0|  India|\n",
      "|63072017|  PR| PRI|  630|72017.0| Barceloneta|         Puerto Rico|            US| 18.445532999999998|-66.56053100000001|Barceloneta, Puer...|1/22/20|        0|     0|  India|\n",
      "|63072019|  PR| PRI|  630|72019.0|Barranquitas|         Puerto Rico|            US|          18.201592|         -66.30963|Barranquitas, Pue...|1/22/20|        0|     0|  India|\n",
      "|63072021|  PR| PRI|  630|72021.0|     Bayamon|         Puerto Rico|            US|           18.34946|        -66.168435|Bayamon, Puerto R...|1/22/20|        0|     0|  India|\n",
      "|63072023|  PR| PRI|  630|72023.0|   Cabo Rojo|         Puerto Rico|            US|          18.040993|        -67.154391|Cabo Rojo, Puerto...|1/22/20|        0|     0|  India|\n",
      "|63072025|  PR| PRI|  630|72025.0|      Caguas|         Puerto Rico|            US|          18.211615|        -66.050779|Caguas, Puerto Ri...|1/22/20|        0|     0|  India|\n",
      "|63072027|  PR| PRI|  630|72027.0|       Camuy|         Puerto Rico|            US|          18.418578|        -66.860206|Camuy, Puerto Ric...|1/22/20|        0|     0|  India|\n",
      "|63072029|  PR| PRI|  630|72029.0|   Canovanas|         Puerto Rico|            US|          18.328802|        -65.887612|Canovanas, Puerto...|1/22/20|        0|     0|  India|\n",
      "|63072031|  PR| PRI|  630|72031.0|    Carolina|         Puerto Rico|            US|          18.374986|-65.95683100000001|Carolina, Puerto ...|1/22/20|        0|     0|  India|\n",
      "|63072033|  PR| PRI|  630|72033.0|      Catano|         Puerto Rico|            US|          18.437269|-66.14330600000001|Catano, Puerto Ri...|1/22/20|        0|     0|  India|\n",
      "+--------+----+----+-----+-------+------------+--------------------+--------------+-------------------+------------------+--------------------+-------+---------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_new.withColumn(\"country\",lit(\"India\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>distinct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+-----+-------+-----------+--------------+--------------+------------------+------------------+--------------------+-------+---------+------+\n",
      "|     UID|iso2|iso3|code3|   FIPS|     Admin2|Province_State|Country_Region|               Lat|             Long_|        Combined_Key|   Date|Confirmed|Deaths|\n",
      "+--------+----+----+-----+-------+-----------+--------------+--------------+------------------+------------------+--------------------+-------+---------+------+\n",
      "|84002060|  US| USA|  840| 2060.0|Bristol Bay|        Alaska|            US|       58.74513976|       -156.701064|Bristol Bay, Alas...|1/22/20|        0|     0|\n",
      "|84012081|  US| USA|  840|12081.0|    Manatee|       Florida|            US|       27.47196942|      -82.31831044|Manatee, Florida, US|1/22/20|        0|     0|\n",
      "|84013301|  US| USA|  840|13301.0|     Warren|       Georgia|            US|       33.41146451|      -82.67795848| Warren, Georgia, US|1/22/20|        0|     0|\n",
      "|84018131|  US| USA|  840|18131.0|    Pulaski|       Indiana|            US|       41.04166107|      -86.69890683|Pulaski, Indiana, US|1/22/20|        0|     0|\n",
      "|84024001|  US| USA|  840|24001.0|   Allegany|      Maryland|            US|       39.62357628|      -78.69280486|Allegany, Marylan...|1/22/20|        0|     0|\n",
      "|84028051|  US| USA|  840|28051.0|     Holmes|   Mississippi|            US|       33.12345854|      -90.08923538|Holmes, Mississip...|1/22/20|        0|     0|\n",
      "|84029007|  US| USA|  840|29007.0|    Audrain|      Missouri|            US|         39.215877|      -91.84242683|Audrain, Missouri...|1/22/20|        0|     0|\n",
      "|84037069|  US| USA|  840|37069.0|   Franklin|North Carolina|            US|         36.081519|      -78.28708884|Franklin, North C...|1/22/20|        0|     0|\n",
      "|84039089|  US| USA|  840|39089.0|    Licking|          Ohio|            US|       40.09136236|      -82.48185785|   Licking, Ohio, US|1/22/20|        0|     0|\n",
      "|84046099|  US| USA|  840|46099.0|  Minnehaha|  South Dakota|            US|       43.67441641|       -96.7910885|Minnehaha, South ...|1/22/20|        0|     0|\n",
      "|84047073|  US| USA|  840|47073.0|    Hawkins|     Tennessee|            US|       36.44355426|      -82.94885298|Hawkins, Tennesse...|1/22/20|        0|     0|\n",
      "|84048239|  US| USA|  840|48239.0|    Jackson|         Texas|            US|       28.95472367|      -96.57749563|  Jackson, Texas, US|1/22/20|        0|     0|\n",
      "|63072055|  PR| PRI|  630|72055.0|    Guanica|   Puerto Rico|            US|         17.982429|-66.91964300000001|Guanica, Puerto R...|1/23/20|        0|     0|\n",
      "|63072081|  PR| PRI|  630|72081.0|      Lares|   Puerto Rico|            US|18.269035000000002|        -66.867236|Lares, Puerto Ric...|1/23/20|        0|     0|\n",
      "|63072147|  PR| PRI|  630|72147.0|    Vieques|   Puerto Rico|            US|         18.123276|-65.44097099999999|Vieques, Puerto R...|1/23/20|        0|     0|\n",
      "|63072153|  PR| PRI|  630|72153.0|      Yauco|   Puerto Rico|            US|         18.080374|        -66.858814|Yauco, Puerto Ric...|1/23/20|        0|     0|\n",
      "|84005021|  US| USA|  840| 5021.0|       Clay|      Arkansas|            US|       36.36826212|       -90.4148172|  Clay, Arkansas, US|1/23/20|        0|     0|\n",
      "|84012055|  US| USA|  840|12055.0|  Highlands|       Florida|            US|       27.34254618|      -81.34071957|Highlands, Florid...|1/23/20|        0|     0|\n",
      "|84016059|  US| USA|  840|16059.0|      Lemhi|         Idaho|            US|        44.9438088|      -113.9309747|    Lemhi, Idaho, US|1/23/20|        0|     0|\n",
      "|84020111|  US| USA|  840|20111.0|       Lyon|        Kansas|            US|       38.45576156|      -96.15275715|    Lyon, Kansas, US|1/23/20|        0|     0|\n",
      "+--------+----+----+-----+-------+-----------+--------------+--------------+------------------+------------------+--------------------+-------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "627920"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.distinct().show()\n",
    "\n",
    "df_new.distinct().count()\n",
    "\n",
    "df_new.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|iso3|code3|\n",
      "+----+-----+\n",
      "| GUM|  316|\n",
      "| USA|  840|\n",
      "| VIR|  850|\n",
      "| ASM|   16|\n",
      "| MNP|  580|\n",
      "| PRI|  630|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.select(\"iso3\",'code3').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|iso3_count_sir|\n",
      "+--------------+\n",
      "|        627920|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df_new.select(count(\"iso3\").alias(\"iso3_count_sir\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+-----+-------+--------+--------------------+--------------+-------------------+------------------+--------------------+-------+---------+------+\n",
      "|     UID|iso2|iso3|code3|   FIPS|  Admin2|      Province_State|Country_Region|                Lat|             Long_|        Combined_Key|   Date|Confirmed|Deaths|\n",
      "+--------+----+----+-----+-------+--------+--------------------+--------------+-------------------+------------------+--------------------+-------+---------+------+\n",
      "|      16|  AS| ASM|   16|   60.0|    NULL|      American Samoa|            US|-14.270999999999999|          -170.132|  American Samoa, US|1/22/20|        0|     0|\n",
      "|     316|  GU| GUM|  316|   66.0|    NULL|                Guam|            US|            13.4443|          144.7937|            Guam, US|1/22/20|        0|     0|\n",
      "|     580|  MP| MNP|  580|   69.0|    NULL|Northern Mariana ...|            US|            15.0979|          145.6739|Northern Mariana ...|1/22/20|        0|     0|\n",
      "|63072001|  PR| PRI|  630|72001.0|Adjuntas|         Puerto Rico|            US| 18.180117000000006|        -66.754367|Adjuntas, Puerto ...|1/22/20|        0|     0|\n",
      "|63072003|  PR| PRI|  630|72003.0|  Aguada|         Puerto Rico|            US|          18.360255|-67.17513100000001|Aguada, Puerto Ri...|1/22/20|        0|     0|\n",
      "+--------+----+----+-----+-------+--------+--------------------+--------------+-------------------+------------------+--------------------+-------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+----+----+-----+-------+--------+--------------------+--------------+-------------------+------------------+-------+---------+------+\n",
      "|     UID|iso2|iso3|code3|   FIPS|  Admin2|      Province_State|Country_Region|                Lat|             Long_|   Date|Confirmed|Deaths|\n",
      "+--------+----+----+-----+-------+--------+--------------------+--------------+-------------------+------------------+-------+---------+------+\n",
      "|      16|  AS| ASM|   16|   60.0|    NULL|      American Samoa|            US|-14.270999999999999|          -170.132|1/22/20|        0|     0|\n",
      "|     316|  GU| GUM|  316|   66.0|    NULL|                Guam|            US|            13.4443|          144.7937|1/22/20|        0|     0|\n",
      "|     580|  MP| MNP|  580|   69.0|    NULL|Northern Mariana ...|            US|            15.0979|          145.6739|1/22/20|        0|     0|\n",
      "|63072001|  PR| PRI|  630|72001.0|Adjuntas|         Puerto Rico|            US| 18.180117000000006|        -66.754367|1/22/20|        0|     0|\n",
      "|63072003|  PR| PRI|  630|72003.0|  Aguada|         Puerto Rico|            US|          18.360255|-67.17513100000001|1/22/20|        0|     0|\n",
      "+--------+----+----+-----+-------+--------+--------------------+--------------+-------------------+------------------+-------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.show(5)\n",
    "\n",
    "df_new.drop(\"Combined_Key\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.select(\"Province_state\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|      Province_state|count|\n",
      "+--------------------+-----+\n",
      "|                Utah| 6956|\n",
      "|              Hawaii| 1316|\n",
      "|           Minnesota|16732|\n",
      "|                Ohio|16920|\n",
      "|Northern Mariana ...|  188|\n",
      "|            Arkansas|14476|\n",
      "|              Oregon| 7144|\n",
      "|               Texas|48128|\n",
      "|        North Dakota|10340|\n",
      "|        Pennsylvania|12972|\n",
      "|         Connecticut| 1880|\n",
      "|            Nebraska|17860|\n",
      "|             Vermont| 3008|\n",
      "|      American Samoa|  188|\n",
      "|              Nevada| 3572|\n",
      "|         Puerto Rico|15040|\n",
      "|          Washington| 7708|\n",
      "|            Illinois|19552|\n",
      "|            Oklahoma|14852|\n",
      "|      Virgin Islands|  188|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.groupBy(\"Province_state\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|      Province_state|          avg_deaths|\n",
      "+--------------------+--------------------+\n",
      "|                Utah|  2.0455721679125936|\n",
      "|              Hawaii|  1.5220364741641337|\n",
      "|           Minnesota|   6.446330384891226|\n",
      "|                Ohio|   13.17677304964539|\n",
      "|Northern Mariana ...|  1.2234042553191489|\n",
      "|            Arkansas|  1.3243299253937553|\n",
      "|              Oregon|  2.5426931690929453|\n",
      "|               Texas|   4.374729886968085|\n",
      "|        North Dakota|  1.2323984526112186|\n",
      "|        Pennsylvania|   41.41697502312673|\n",
      "|         Connecticut|  199.12021276595743|\n",
      "|            Nebraska|  1.0100223964165733|\n",
      "|             Vermont|  1.9321808510638299|\n",
      "|      American Samoa|                 0.0|\n",
      "|              Nevada|  12.620940649496081|\n",
      "|         Puerto Rico|1.329787234042553...|\n",
      "|          Washington|   16.20329527763363|\n",
      "|            Illinois|   27.70417348608838|\n",
      "|            Oklahoma|  2.3882305413412337|\n",
      "|      Virgin Islands|  3.0372340425531914|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Admin2',\n",
       " 'Combined_Key',\n",
       " 'Confirmed',\n",
       " 'Country_Region',\n",
       " 'Date',\n",
       " 'Deaths',\n",
       " 'FIPS',\n",
       " 'Lat',\n",
       " 'Long_',\n",
       " 'Province_State',\n",
       " 'UID',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_collect_as_arrow',\n",
       " '_ipython_key_completions_',\n",
       " '_jcols',\n",
       " '_jdf',\n",
       " '_jmap',\n",
       " '_joinAsOf',\n",
       " '_jseq',\n",
       " '_lazy_rdd',\n",
       " '_repr_html_',\n",
       " '_sc',\n",
       " '_schema',\n",
       " '_session',\n",
       " '_show_string',\n",
       " '_sort_cols',\n",
       " '_sql_ctx',\n",
       " '_support_repr_html',\n",
       " 'agg',\n",
       " 'alias',\n",
       " 'approxQuantile',\n",
       " 'cache',\n",
       " 'checkpoint',\n",
       " 'coalesce',\n",
       " 'code3',\n",
       " 'colRegex',\n",
       " 'collect',\n",
       " 'columns',\n",
       " 'corr',\n",
       " 'count',\n",
       " 'cov',\n",
       " 'createGlobalTempView',\n",
       " 'createOrReplaceGlobalTempView',\n",
       " 'createOrReplaceTempView',\n",
       " 'createTempView',\n",
       " 'crossJoin',\n",
       " 'crosstab',\n",
       " 'cube',\n",
       " 'describe',\n",
       " 'distinct',\n",
       " 'drop',\n",
       " 'dropDuplicates',\n",
       " 'dropDuplicatesWithinWatermark',\n",
       " 'drop_duplicates',\n",
       " 'dropna',\n",
       " 'dtypes',\n",
       " 'exceptAll',\n",
       " 'explain',\n",
       " 'fillna',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'foreach',\n",
       " 'foreachPartition',\n",
       " 'freqItems',\n",
       " 'groupBy',\n",
       " 'groupby',\n",
       " 'head',\n",
       " 'hint',\n",
       " 'inputFiles',\n",
       " 'intersect',\n",
       " 'intersectAll',\n",
       " 'isEmpty',\n",
       " 'isLocal',\n",
       " 'isStreaming',\n",
       " 'is_cached',\n",
       " 'iso2',\n",
       " 'iso3',\n",
       " 'join',\n",
       " 'limit',\n",
       " 'localCheckpoint',\n",
       " 'mapInArrow',\n",
       " 'mapInPandas',\n",
       " 'melt',\n",
       " 'na',\n",
       " 'observe',\n",
       " 'offset',\n",
       " 'orderBy',\n",
       " 'pandas_api',\n",
       " 'persist',\n",
       " 'printSchema',\n",
       " 'randomSplit',\n",
       " 'rdd',\n",
       " 'registerTempTable',\n",
       " 'repartition',\n",
       " 'repartitionByRange',\n",
       " 'replace',\n",
       " 'rollup',\n",
       " 'sameSemantics',\n",
       " 'sample',\n",
       " 'sampleBy',\n",
       " 'schema',\n",
       " 'select',\n",
       " 'selectExpr',\n",
       " 'semanticHash',\n",
       " 'show',\n",
       " 'sort',\n",
       " 'sortWithinPartitions',\n",
       " 'sparkSession',\n",
       " 'sql_ctx',\n",
       " 'stat',\n",
       " 'storageLevel',\n",
       " 'subtract',\n",
       " 'summary',\n",
       " 'tail',\n",
       " 'take',\n",
       " 'to',\n",
       " 'toDF',\n",
       " 'toJSON',\n",
       " 'toLocalIterator',\n",
       " 'toPandas',\n",
       " 'to_koalas',\n",
       " 'to_pandas_on_spark',\n",
       " 'transform',\n",
       " 'union',\n",
       " 'unionAll',\n",
       " 'unionByName',\n",
       " 'unpersist',\n",
       " 'unpivot',\n",
       " 'where',\n",
       " 'withColumn',\n",
       " 'withColumnRenamed',\n",
       " 'withColumns',\n",
       " 'withColumnsRenamed',\n",
       " 'withMetadata',\n",
       " 'withWatermark',\n",
       " 'write',\n",
       " 'writeStream',\n",
       " 'writeTo']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df_new.groupBy('Province_state').agg(avg(\"Deaths\").alias(\"avg_deaths\")).show()\n",
    "dir(df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>rollup -->Creates subtotal + grand total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|Province_state|sum(Deaths)|\n",
      "+--------------+-----------+\n",
      "|        Nevada|      45082|\n",
      "|Virgin Islands|        571|\n",
      "| New Hampshire|      25438|\n",
      "+--------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df_new.rollup(\"Province_state\").sum(\"Deaths\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Mulitiple Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+------------+\n",
      "|      Province_state|          Avg_Deaths|max_Deaths|Total_Deaths|\n",
      "+--------------------+--------------------+----------+------------+\n",
      "|                Utah|  2.0455721679125936|       160|       14229|\n",
      "|              Hawaii|  1.5220364741641337|        47|        2003|\n",
      "|           Minnesota|   6.446330384891226|       807|      107860|\n",
      "|                Ohio|   13.17677304964539|       494|      222951|\n",
      "|Northern Mariana ...|  1.2234042553191489|         2|         230|\n",
      "|            Arkansas|  1.3243299253937553|        71|       19171|\n",
      "|              Oregon|  2.5426931690929453|        82|       18165|\n",
      "|               Texas|   4.374729886968085|       655|      210547|\n",
      "|        North Dakota|  1.2323984526112186|      1059|       12743|\n",
      "|        Pennsylvania|   41.41697502312673|      1678|      537261|\n",
      "|         Connecticut|  199.12021276595743|      1410|      374346|\n",
      "|            Nebraska|  1.0100223964165733|       121|       18039|\n",
      "|             Vermont|  1.9321808510638299|        81|        5812|\n",
      "|      American Samoa|                 0.0|         0|           0|\n",
      "|              Nevada|  12.620940649496081|       606|       45082|\n",
      "|         Puerto Rico|1.329787234042553...|         1|           2|\n",
      "|          Washington|   16.20329527763363|       645|      124895|\n",
      "|            Illinois|   27.70417348608838|      4842|      541672|\n",
      "|            Oklahoma|  2.3882305413412337|        90|       35470|\n",
      "|      Virgin Islands|  3.0372340425531914|         7|         571|\n",
      "+--------------------+--------------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg,sum,max\n",
    "\n",
    "df.groupBy(\"Province_state\").agg(avg(\"Deaths\").alias(\"Avg_Deaths\"), max(\"Deaths\").alias(\"max_Deaths\"),sum(\"Deaths\").alias(\"Total_Deaths\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|          WHO Region|        Avg Deaths|\n",
      "+--------------------+------------------+\n",
      "|              Europe|3770.4285714285716|\n",
      "|     Western Pacific|          515.5625|\n",
      "|              Africa|254.64583333333334|\n",
      "|Eastern Mediterra...|1742.6818181818182|\n",
      "|            Americas| 9792.342857142858|\n",
      "|     South-East Asia|            4134.9|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3=spark.read.csv(r\"C:\\Users\\Lenovo\\Downloads\\dyd\\data\\country_wise_latest.csv\",header=True,inferSchema=True)\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df3.groupBy('WHO Region').agg(avg('Deaths').alias(\"Avg Deaths\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable       Type            Data/Info\n",
      "----------------------------------------\n",
      "Row            type            <class 'pyspark.sql.types.Row'>\n",
      "SparkSession   type            <class 'pyspark.sql.session.SparkSession'>\n",
      "avg            function        <function avg at 0x000002B3FE9468E0>\n",
      "col            function        <function col at 0x000002B3FE945260>\n",
      "columns        list            n=2\n",
      "count          function        <function count at 0x000002B3FE946660>\n",
      "data1          list            n=2\n",
      "df             DataFrame       DataFrame[UID: int, iso2:<...>firmed: int, Deaths: int]\n",
      "df1            DataFrame       DataFrame[Name: string, Age: bigint]\n",
      "df2            DataFrame       DataFrame[UID: int, iso2:<...> int, updated_code3: int]\n",
      "df3            DataFrame       DataFrame[Country/Region:<...>uble, WHO Region: string]\n",
      "df_new         DataFrame       DataFrame[UID: int, iso2:<...>firmed: int, Deaths: int]\n",
      "lit            function        <function lit at 0x000002B3FE945120>\n",
      "max            function        <function max at 0x000002B3FE946160>\n",
      "spark          SparkSession    <pyspark.sql.session.Spar<...>ct at 0x000002B3FE9BBD10>\n",
      "sum            function        <function sum at 0x000002B3FE9467A0>\n",
      "when           function        <function when at 0x000002B3FE964900>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def list_dataframes(scope=globals()):\n",
    "    print(\" Available DataFrames:\")\n",
    "    found = False\n",
    "    for var_name, var_val in scope.items():\n",
    "        if isinstance(var_val, DataFrame):\n",
    "            found = True\n",
    "            print(f\" {var_name} | Rows: {var_val.count()} | Columns: {len(var_val.columns)}\")\n",
    "    if not found:\n",
    "        print(\" No DataFrames found in this scope.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df1 = spark.range(10)\n",
    "# # df2 = spark.createDataFrame([(1, 'Alice'), (2, 'Bob')], ['id', 'name'])\n",
    "\n",
    "# list_dataframes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df\n",
      "_11\n",
      "df1\n",
      "df3\n",
      "df_new\n",
      "df2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "only_dfs = {name: val for name, val in globals().items() if isinstance(val, DataFrame)}\n",
    "\n",
    "for name in only_dfs:\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Joins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+----------+----------+--------+-----------+---------+--------------+------------+-----------+----------------+----------------+-------------+----------+------------+----------+\n",
      "|Country/Region|    Continent|Population|TotalCases|NewCases|TotalDeaths|NewDeaths|TotalRecovered|NewRecovered|ActiveCases|Serious,Critical|Tot Cases/1M pop|Deaths/1M pop|TotalTests|Tests/1M pop|WHO Region|\n",
      "+--------------+-------------+----------+----------+--------+-----------+---------+--------------+------------+-----------+----------------+----------------+-------------+----------+------------+----------+\n",
      "|           USA|North America| 331198130|   5032179|    NULL|     162804|     NULL|       2576668|        NULL|    2292707|           18296|           15194|        492.0|  63139605|      190640|  Americas|\n",
      "|        Brazil|South America| 212710692|   2917562|    NULL|      98644|     NULL|       2047660|        NULL|     771258|            8318|           13716|        464.0|  13206188|       62085|  Americas|\n",
      "+--------------+-------------+----------+----------+--------+-----------+---------+--------------+------------+-----------+----------------+----------------+-------------+----------+------------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------+---------+------+---------+------+---------+----------+-------------+------------------+---------------------+----------------------+-------------------+-------------+-----------------+--------------------+\n",
      "|Country/Region|Confirmed|Deaths|Recovered|Active|New cases|New deaths|New recovered|Deaths / 100 Cases|Recovered / 100 Cases|Deaths / 100 Recovered|Confirmed last week|1 week change|1 week % increase|          WHO Region|\n",
      "+--------------+---------+------+---------+------+---------+----------+-------------+------------------+---------------------+----------------------+-------------------+-------------+-----------------+--------------------+\n",
      "|   Afghanistan|    36263|  1269|    25198|  9796|      106|        10|           18|               3.5|                69.49|                  5.04|              35526|          737|             2.07|Eastern Mediterra...|\n",
      "|       Albania|     4880|   144|     2745|  1991|      117|         6|           63|              2.95|                56.25|                  5.25|               4171|          709|             17.0|              Europe|\n",
      "+--------------+---------+------+---------+------+---------+----------+-------------+------------------+---------------------+----------------------+-------------------+-------------+-----------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName(\"joins\").getOrCreate()\n",
    "\n",
    "df_worldom=spark.read.csv(r\"C:\\Users\\Lenovo\\Downloads\\dyd\\data\\worldometer_data.csv\",header=True,inferSchema=True)\n",
    "\n",
    "df_countrywise=spark.read.csv(r\"C:\\Users\\Lenovo\\Downloads\\dyd\\data\\country_wise_latest.csv\",header=True,inferSchema=True)\n",
    "\n",
    "df_worldom.show(2)\n",
    "df_countrywise.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+------------------+\n",
      "|Country|Population|TotalCases|     InfectionRate|\n",
      "+-------+----------+----------+------------------+\n",
      "|  Qatar|   2807805|    112092|3.9921575750452756|\n",
      "|Bahrain|   1706669|     42889|2.5130239079751258|\n",
      "|  Chile|  19132514|    366671|1.9164810228284688|\n",
      "| Panama|   4321282|     71418|1.6527039892328248|\n",
      "| Kuwait|   4276658|     70045|1.6378443167538765|\n",
      "+-------+----------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Show the top 5 countries (from both datasets) with the highest percentage of population infected. Include total cases and population,\n",
    "# and only show countries with more than 10 million population. \n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_worldom=df_worldom.alias(\"a\")\n",
    "df_countrywise=df_countrywise.alias(\"b\")\n",
    "\n",
    "df_joined=df_worldom.join(df_countrywise,col(\"a.Country/Region\")== col(\"b.Country/Region\"),\"inner\")\n",
    "\n",
    "df_joined.filter(col(\"a.Population\")>1000000)\\\n",
    ".withColumn(\"InfectionRate\",(col(\"a.TotalCases\")*100 / col(\"a.Population\")))\\\n",
    ".select(col(\"a.Country/Region\").alias(\"Country\"),\\\n",
    "col(\"a.Population\"),\\\n",
    "col(\"a.TotalCases\"),\\\n",
    "col(\"InfectionRate\"))\\\n",
    ".orderBy(col(\"InfectionRate\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+------+---------+------+---------+----------+-------------+------------------+---------------------+----------------------+-------------------+-------------+-----------------+--------------------+\n",
      "|Country/Region|Confirmed|Deaths|Recovered|Active|New cases|New deaths|New recovered|Deaths / 100 Cases|Recovered / 100 Cases|Deaths / 100 Recovered|Confirmed last week|1 week change|1 week % increase|          WHO Region|\n",
      "+--------------+---------+------+---------+------+---------+----------+-------------+------------------+---------------------+----------------------+-------------------+-------------+-----------------+--------------------+\n",
      "|   Afghanistan|    36263|  1269|    25198|  9796|      106|        10|           18|               3.5|                69.49|                  5.04|              35526|          737|             2.07|Eastern Mediterra...|\n",
      "+--------------+---------+------+---------+------+---------+----------+-------------+------------------+---------------------+----------------------+-------------------+-------------+-----------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------+-------------+----------+----------+--------+-----------+---------+--------------+------------+-----------+----------------+----------------+-------------+----------+------------+----------+\n",
      "|Country/Region|    Continent|Population|TotalCases|NewCases|TotalDeaths|NewDeaths|TotalRecovered|NewRecovered|ActiveCases|Serious,Critical|Tot Cases/1M pop|Deaths/1M pop|TotalTests|Tests/1M pop|WHO Region|\n",
      "+--------------+-------------+----------+----------+--------+-----------+---------+--------------+------------+-----------+----------------+----------------+-------------+----------+------------+----------+\n",
      "|           USA|North America| 331198130|   5032179|    NULL|     162804|     NULL|       2576668|        NULL|    2292707|           18296|           15194|        492.0|  63139605|      190640|  Americas|\n",
      "|        Brazil|South America| 212710692|   2917562|    NULL|      98644|     NULL|       2047660|        NULL|     771258|            8318|           13716|        464.0|  13206188|       62085|  Americas|\n",
      "+--------------+-------------+----------+----------+--------+-----------+---------+--------------+------------+-----------+----------------+----------------+-------------+----------+------------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------------+----------+------------------+\n",
      "| Country/Region|TotalCases|     Death_rate(%)|\n",
      "+---------------+----------+------------------+\n",
      "|         France|    195633|15.494318443207435|\n",
      "|             UK|    308134| 15.06260263391901|\n",
      "|          Italy|    249204|14.119757307266337|\n",
      "|        Belgium|     71158|13.855083054610866|\n",
      "|         Mexico|    462690|10.918109317253453|\n",
      "|    Netherlands|     56982|10.798146783194692|\n",
      "|   Sint Maarten|       160|              10.0|\n",
      "|          Spain|    354530| 8.038811948213127|\n",
      "|Channel Islands|       597| 7.872696817420436|\n",
      "|     Montserrat|        13|7.6923076923076925|\n",
      "|         Canada|    118561| 7.562351869501775|\n",
      "|    Isle of Man|       336| 7.142857142857143|\n",
      "|         Sweden|     81967| 7.034538289799554|\n",
      "|        Ireland|     26372| 6.704080084938571|\n",
      "|        Ecuador|     90537| 6.491268763047152|\n",
      "|     San Marino|       699| 6.008583690987124|\n",
      "|        Bermuda|       157| 5.732484076433121|\n",
      "|           Iran|    320117| 5.615446852244648|\n",
      "|        Andorra|       944| 5.508474576271187|\n",
      "|    Switzerland|     36108| 5.497396698792511|\n",
      "+---------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List countries where the number of deaths per million exceeds the average deaths per million (globally), \n",
    "# and show total cases and death rate, ordered by death rate descending.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_countrywise.show(1)\n",
    "\n",
    "df_worldom.show(2)\n",
    "\n",
    "df_worldom=df_worldom.alias(\"a\")\n",
    "df_countrywise=df_countrywise.alias(\"b\")\n",
    "\n",
    "df_join=df_worldom.join(df_countrywise,col(\"a.Country/Region\") == col(\"b.Country/Region\"))\n",
    "\n",
    "df_global_avg=df_worldom.select(avg(col(\"a.Deaths/1M pop\"))).collect()[0][0]\n",
    "\n",
    "df_filtered=df_worldom.filter(col(\"a.Deaths/1M pop\") > df_global_avg)\n",
    "\n",
    "df_death_rate=df_filtered.withColumn(\"Death_rate(%)\",col(\"a.TotalDeaths\")*100 /col(\"a.TotalCases\"))\n",
    "\n",
    "df_result=df_death_rate.select(col(\"a.Country/Region\"),col(\"a.TotalCases\"),col(\"Death_rate(%)\")).orderBy(col(\"Death_rate(%)\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Window Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "Window.partitionBy(\"col1\").orderBy(\"col2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+---------+\n",
      "|     Country/Region|      Date|Confirmed|\n",
      "+-------------------+----------+---------+\n",
      "|        Afghanistan|2020-07-27|    36263|\n",
      "|            Albania|2020-07-27|     4880|\n",
      "|            Algeria|2020-07-27|    27973|\n",
      "|            Andorra|2020-07-27|      907|\n",
      "|             Angola|2020-07-27|      950|\n",
      "|Antigua and Barbuda|2020-07-27|       86|\n",
      "|          Argentina|2020-07-27|   167416|\n",
      "|            Armenia|2020-07-27|    37390|\n",
      "|          Australia|2020-07-27|      113|\n",
      "|            Austria|2020-07-27|    20558|\n",
      "|         Azerbaijan|2020-07-27|    30446|\n",
      "|            Bahamas|2020-07-27|      382|\n",
      "|            Bahrain|2020-07-27|    39482|\n",
      "|         Bangladesh|2020-07-27|   226225|\n",
      "|           Barbados|2020-07-27|      110|\n",
      "|            Belarus|2020-07-27|    67251|\n",
      "|            Belgium|2020-07-27|    66428|\n",
      "|             Belize|2020-07-27|       48|\n",
      "|              Benin|2020-07-27|     1770|\n",
      "|             Bhutan|2020-07-27|       99|\n",
      "+-------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each country,\n",
    "#  list the most recent date when COVID cases were reported (based on date column),\n",
    "#  and show the number of confirmed cases on that day.\n",
    "\n",
    "\n",
    "df_covid_19=spark.read.csv(r\"C:\\Users\\Lenovo\\Downloads\\dyd\\data\\covid_19_clean_complete.csv\",header=True,inferSchema=True)\n",
    "# df_worldom.show(1)\n",
    "# df_countrywise.show(1)\n",
    "# df_covid_19.show(1)\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "\n",
    "# Define window spec: partition by country, order by date descending (latest first)\n",
    "window_spec = Window.partitionBy(\"Country/Region\").orderBy(col(\"Date\").desc())\n",
    "\n",
    "# Apply row_number and filter to only keep the latest row per country\n",
    "latest_cases = df_covid_19.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"row_num\") == 1) \\\n",
    "    .select(\"Country/Region\", \"Date\", \"Confirmed\")\n",
    "\n",
    "latest_cases.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+----+\n",
      "|Country/Region|Confirmed|rank|\n",
      "+--------------+---------+----+\n",
      "|            US|  4290259|   1|\n",
      "|        Brazil|  2442375|   2|\n",
      "|         India|  1480073|   3|\n",
      "|        Russia|   816680|   4|\n",
      "|  South Africa|   452529|   5|\n",
      "|        Mexico|   395489|   6|\n",
      "|          Peru|   389717|   7|\n",
      "|         Chile|   347923|   8|\n",
      "|United Kingdom|   301708|   9|\n",
      "|          Iran|   293606|  10|\n",
      "|      Pakistan|   274289|  11|\n",
      "|         Spain|   272421|  12|\n",
      "|  Saudi Arabia|   268934|  13|\n",
      "|      Colombia|   257101|  14|\n",
      "|         Italy|   246286|  15|\n",
      "|        Turkey|   227019|  16|\n",
      "|    Bangladesh|   226225|  17|\n",
      "|        France|   220352|  18|\n",
      "|       Germany|   207112|  19|\n",
      "|     Argentina|   167416|  20|\n",
      "+--------------+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Rank countries by total confirmed cases globally.\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "window_spec = Window.orderBy(col(\"Confirmed\").desc())\n",
    "\n",
    "df_countrywise.withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "    .select(\"Country/Region\", \"Confirmed\", \"rank\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+---------+------------------+----------+\n",
      "|Country/Region|Deaths|Confirmed|         DeathRate|dense_rank|\n",
      "+--------------+------+---------+------------------+----------+\n",
      "|         Yemen|   483|     1691|28.562980484920164|         1|\n",
      "|United Kingdom| 45844|   301708|15.194824134593713|         2|\n",
      "|       Belgium|  9822|    66428|14.785933642439934|         3|\n",
      "|         Italy| 35112|   246286|14.256595990027854|         4|\n",
      "|        France| 30212|   220352|13.710790008713332|         5|\n",
      "|       Hungary|   596|     4448|13.399280575539569|         6|\n",
      "|   Netherlands|  6160|    53413|11.532772920450078|         7|\n",
      "|        Mexico| 44022|   395489|11.131030193001575|         8|\n",
      "|         Spain| 28432|   272421|10.436787178668311|         9|\n",
      "|Western Sahara|     1|       10|              10.0|        10|\n",
      "|          Chad|    75|      922| 8.134490238611713|        11|\n",
      "|        Canada|  8944|   116458| 7.680021982173831|        12|\n",
      "|        Sweden|  5700|    79395| 7.179293406385793|        13|\n",
      "|       Ecuador|  5532|    81161|6.8160816155542685|        14|\n",
      "|       Ireland|  1764|    25892| 6.812915186157887|        15|\n",
      "|      Barbados|     7|      110| 6.363636363636363|        16|\n",
      "|         Sudan|   720|    11424| 6.302521008403361|        17|\n",
      "|       Liberia|    72|     1167| 6.169665809768637|        18|\n",
      "|         Niger|    69|     1132| 6.095406360424028|        19|\n",
      "|    San Marino|    42|      699| 6.008583690987124|        20|\n",
      "+--------------+------+---------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Rank countries by death rate (TotalDeaths / TotalCases * 100)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import dense_rank, expr\n",
    "\n",
    "df_with_rate = df_countrywise.withColumn(\"DeathRate\", (col(\"Deaths\") * 100) / col(\"Confirmed\"))\n",
    "window_spec = Window.orderBy(col(\"DeathRate\").desc())\n",
    "\n",
    "df_with_rate.withColumn(\"dense_rank\", dense_rank().over(window_spec)) \\\n",
    "    .select(\"Country/Region\", \"Deaths\", \"Confirmed\", \"DeathRate\", \"dense_rank\") \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+---------+-------------+-----------+\n",
      "|Country/Region|      Date|Confirmed|PreviousCases|DailyChange|\n",
      "+--------------+----------+---------+-------------+-----------+\n",
      "|   Afghanistan|2020-01-22|        0|         NULL|       NULL|\n",
      "|   Afghanistan|2020-01-23|        0|            0|          0|\n",
      "|   Afghanistan|2020-01-24|        0|            0|          0|\n",
      "|   Afghanistan|2020-01-25|        0|            0|          0|\n",
      "|   Afghanistan|2020-01-26|        0|            0|          0|\n",
      "|   Afghanistan|2020-01-27|        0|            0|          0|\n",
      "|   Afghanistan|2020-01-28|        0|            0|          0|\n",
      "|   Afghanistan|2020-01-29|        0|            0|          0|\n",
      "|   Afghanistan|2020-01-30|        0|            0|          0|\n",
      "|   Afghanistan|2020-01-31|        0|            0|          0|\n",
      "|   Afghanistan|2020-02-01|        0|            0|          0|\n",
      "|   Afghanistan|2020-02-02|        0|            0|          0|\n",
      "|   Afghanistan|2020-02-03|        0|            0|          0|\n",
      "|   Afghanistan|2020-02-04|        0|            0|          0|\n",
      "|   Afghanistan|2020-02-05|        0|            0|          0|\n",
      "|   Afghanistan|2020-02-06|        0|            0|          0|\n",
      "|   Afghanistan|2020-02-07|        0|            0|          0|\n",
      "|   Afghanistan|2020-02-08|        0|            0|          0|\n",
      "|   Afghanistan|2020-02-09|        0|            0|          0|\n",
      "|   Afghanistan|2020-02-10|        0|            0|          0|\n",
      "+--------------+----------+---------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#For each country, calculate the day-over-day change in confirmed cases.\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "window_spec = Window.partitionBy(\"Country/Region\").orderBy(\"Date\")\n",
    "\n",
    "df_covid_19.withColumn(\"PreviousCases\", lag(\"Confirmed\", 1).over(window_spec)) \\\n",
    "    .withColumn(\"DailyChange\", col(\"Confirmed\") - col(\"PreviousCases\")) \\\n",
    "    .select(\"Country/Region\", \"Date\", \"Confirmed\", \"PreviousCases\", \"DailyChange\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+------+-------------+\n",
      "|Country/Region|      Date|Deaths|NextDayDeaths|\n",
      "+--------------+----------+------+-------------+\n",
      "|   Afghanistan|2020-01-22|     0|            0|\n",
      "|   Afghanistan|2020-01-23|     0|            0|\n",
      "|   Afghanistan|2020-01-24|     0|            0|\n",
      "|   Afghanistan|2020-01-25|     0|            0|\n",
      "|   Afghanistan|2020-01-26|     0|            0|\n",
      "|   Afghanistan|2020-01-27|     0|            0|\n",
      "|   Afghanistan|2020-01-28|     0|            0|\n",
      "|   Afghanistan|2020-01-29|     0|            0|\n",
      "|   Afghanistan|2020-01-30|     0|            0|\n",
      "|   Afghanistan|2020-01-31|     0|            0|\n",
      "|   Afghanistan|2020-02-01|     0|            0|\n",
      "|   Afghanistan|2020-02-02|     0|            0|\n",
      "|   Afghanistan|2020-02-03|     0|            0|\n",
      "|   Afghanistan|2020-02-04|     0|            0|\n",
      "|   Afghanistan|2020-02-05|     0|            0|\n",
      "|   Afghanistan|2020-02-06|     0|            0|\n",
      "|   Afghanistan|2020-02-07|     0|            0|\n",
      "|   Afghanistan|2020-02-08|     0|            0|\n",
      "|   Afghanistan|2020-02-09|     0|            0|\n",
      "|   Afghanistan|2020-02-10|     0|            0|\n",
      "+--------------+----------+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#For each country, identify the next day's projection of deaths.\n",
    "\n",
    "from pyspark.sql.functions import lead\n",
    "\n",
    "window_spec = Window.partitionBy(\"Country/Region\").orderBy(\"Date\")\n",
    "\n",
    "df_covid_19.withColumn(\"NextDayDeaths\", lead(\"Deaths\", 1).over(window_spec)) \\\n",
    "    .select(\"Country/Region\", \"Date\", \"Deaths\", \"NextDayDeaths\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+--------+\n",
      "|Country/Region|Confirmed|Quartile|\n",
      "+--------------+---------+--------+\n",
      "|       Namibia|     1843|       3|\n",
      "|  Sierra Leone|     1783|       3|\n",
      "|         Benin|     1770|       3|\n",
      "|    Mozambique|     1701|       3|\n",
      "|         Yemen|     1691|       3|\n",
      "|   New Zealand|     1557|       3|\n",
      "|      Suriname|     1483|       3|\n",
      "|       Tunisia|     1455|       3|\n",
      "|        Latvia|     1219|       3|\n",
      "|       Uruguay|     1202|       3|\n",
      "|        Jordan|     1176|       3|\n",
      "|       Liberia|     1167|       3|\n",
      "|       Georgia|     1137|       3|\n",
      "|         Niger|     1132|       3|\n",
      "|        Uganda|     1128|       3|\n",
      "|  Burkina Faso|     1100|       3|\n",
      "|        Cyprus|     1060|       4|\n",
      "|        Angola|      950|       4|\n",
      "|          Chad|      922|       4|\n",
      "|       Andorra|      907|       4|\n",
      "+--------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Divide countries into 4 quartiles based on total confirmed cases.\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import ntile\n",
    "\n",
    "window_spec = Window.orderBy(col(\"Confirmed\").desc())\n",
    "\n",
    "df_countrywise.withColumn(\"Quartile\", ntile(4).over(window_spec)) \\\n",
    "    .select(\"Country/Region\", \"Confirmed\", \"Quartile\").offset(125) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
